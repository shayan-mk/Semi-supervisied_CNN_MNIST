{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shayan-mk/Semi-supervisied_CNN_MNIST/blob/master/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "t2TQIRNrOVdO"
      },
      "source": [
        "# Section 0: Introduction"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Abstract:\n",
        "\n",
        "The objective of this project is to demonstrate the effect of semi-supervised image classification over its fully supervised counterpart. Using the MNIST dataset of handwritten digits, the motivation being to show the benefits of having unlabeled data points in the training dataset. Using a convolutional neural network trained over a subset of the labeled MNIST dataset, we leverage its learned associations as an encoder to extract deep features from the remaining subset of the labeled dataset. In the CNN, we use a series of convolutional layers to expose low level features before flattening them for the linear classifier. Applying unsupervised K-means clustering over the unlabeled subset of training data, we introduce a clustering loss factor to these data points in relation to the ground truth label. Using the K-nearest neighbors algorithms, we propagate the set of assignable labels (0-9) over the 10 clusters. The CNN is then trained over the full set of ground-truth and generated-label datapoints. The goal then is to show the varying degrees of effectiveness between fully labeled and different ratios of unlabeled-to-labeled datasets. Illustrated and graphed by the change in loss and accuracy at each batch across all epochs for both the training and test MNIST datasets."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Team members and contributions:\n",
        "\n",
        "- Bowen Luo (b23luo@uwaterloo.ca)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "M4iBebg4ut-R"
      },
      "source": [
        "# Code outline:\n",
        "\n",
        "- Apply data augmentation techniques to expand the training dataset with rotation, cropping, blurring, and noise transformations to reduce overfitting.\n",
        "- Train the initial fully supervised convolutional neural network on the labeled subset of MNIST data.\n",
        "- Extract deep features from the last layer before the linear classifier for the unlabeled data points.\n",
        "- Perform K-means clustering on the deep features to assign cluster labels to the unlabeled data points.\n",
        "- Use K-Nearest Neighbors (KNN) on the deep features to further refine the labels assigned by K-means clustering based on the labels of their nearest neighbors in the feature space.\n",
        "- Combine the labeled, K-means labeled, and KNN-refined labeled data points into a single dataset.\n",
        "- Fine-tune the neural network using this dataset with the combined loss function.\n",
        "\n",
        "Finally, Evaluate the model performance with varying a variying ratio of labeled data to demonstrate how the performance changes as the ratio gets progressively smaller. We will do this by repeating the above steps for different proportions of the training MNIST dataset."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FaCfyyhLIFYY"
      },
      "source": [
        "# Section 1: Code libraries\n",
        "\n",
        "Aside from essential libraries such as math and numpy, pytorch is used for its tensors, transforms, dataset objects, as well as general CNN related functions including loss functions, convolutional layers, and forward/backward passes over the CNN. Matplotlib is used to graph the loss and accuracies over the batches, sklearn is used for K-means clustering, and we use KeOps for its conversion of tensors into symbolic variables for low overhead matrix operations for K-nearest-neighbors.\n",
        "\n",
        "Note: running on colab <strong>using GPU</strong>, the following code cell is included for installation of KeOps (pykeops). Locally, it requires the CUDA toolkit and compatible g++ compiler. Further installation requirements can be found here: https://www.kernel-operations.io/keops/python/installation.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MGJh5-zaTa3",
        "outputId": "43fa0e74-7c02-4cef-d91a-4f2150d51494"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pykeops\n",
            "  Downloading pykeops-2.1.2.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.9/88.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pykeops) (1.22.4)\n",
            "Collecting pybind11\n",
            "  Downloading pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.3/222.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keopscore==2.1.2\n",
            "  Downloading keopscore-2.1.2.tar.gz (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pykeops, keopscore\n",
            "  Building wheel for pykeops (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pykeops: filename=pykeops-2.1.2-py3-none-any.whl size=114095 sha256=0b4de0d03785904f81ffb5c7a90aa020a10d7e5444c11d10112e8d92ef685daf\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/a1/b9/156d48595487f5df04d1c5eee5ec01f3330ae2debece7ca6fe\n",
            "  Building wheel for keopscore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keopscore: filename=keopscore-2.1.2-py3-none-any.whl size=146469 sha256=175c2f4ac9bb8997a60f87b47c2f204f51bff3652d3b8158e4557d8cac9f7d4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/53/76/4a583149d55824e0d0a30a303cc5cd6ae1d4545d3f61f346c9\n",
            "Successfully built pykeops keopscore\n",
            "Installing collected packages: pybind11, keopscore, pykeops\n",
            "Successfully installed keopscore-2.1.2 pybind11-2.10.4 pykeops-2.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pykeops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcqxDZyYIatw",
        "outputId": "c45aeba2-07fd-4bf2-8d07-5bec3fef37b9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statistics import mode\n",
        "from sklearn.cluster import KMeans\n",
        "#from pykeops.torch import LazyTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8ZdSNfgSLaVv"
      },
      "outputs": [],
      "source": [
        "USE_GPU = True\n",
        "EPOCH = 2\n",
        "BATCH = 60\n",
        "LABELED_RATIO = 0.2\n",
        "DEVICE = \"cpu\"\n",
        "USE_SYMBOLIC = False\n",
        "CLUSTER_RATIO = 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VZftz1ACut-U"
      },
      "outputs": [],
      "source": [
        "def set_device():\n",
        "    global DEVICE\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-P_1ocVIonk"
      },
      "source": [
        "# Section 2: Define the CNN architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GC8Bd7ouJFUe"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    # create convolutional and batch layers 1 through 8 in constructor\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, 3, padding=1)\n",
        "        self.conv1_bn = nn.BatchNorm2d(64)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv2_bn = nn.BatchNorm2d(128)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.conv3_bn = nn.BatchNorm2d(256)\n",
        "        \n",
        "        self.conv4 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "        self.conv4_bn = nn.BatchNorm2d(256)\n",
        "        \n",
        "        self.conv5 = nn.Conv2d(256, 512, 3, padding=1)\n",
        "        self.conv5_bn = nn.BatchNorm2d(512)\n",
        "        \n",
        "        self.conv6 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "        self.conv6_bn = nn.BatchNorm2d(512)\n",
        "        \n",
        "        self.conv7 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "        self.conv7_bn = nn.BatchNorm2d(512)\n",
        "        \n",
        "        self.conv8 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "        self.conv8_bn = nn.BatchNorm2d(512)\n",
        "        # create fully connected layers and dropout\n",
        "        self.fc1 = nn.Linear(512, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, 10)\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    # define forward pass through network, conversely, could use nn.sequential\n",
        "    def forward(self, x, extract_features=False):\n",
        "        x = F.max_pool2d(F.relu(self.conv1_bn(self.conv1(x))), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2_bn(self.conv2(x))), (2, 2))\n",
        "        x = F.relu(self.conv3_bn(self.conv3(x)))\n",
        "        x = F.max_pool2d(F.relu(self.conv4_bn(self.conv4(x))), (2, 2))\n",
        "        x = F.relu(self.conv5_bn(self.conv5(x)))\n",
        "        x = F.max_pool2d(F.relu(self.conv6_bn(self.conv6(x))), (2, 2))\n",
        "        x = F.relu(self.conv7_bn(self.conv7(x)))\n",
        "        x = F.max_pool2d(F.relu(self.conv8_bn(self.conv8(x))), (2, 2))\n",
        "        \n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        if extract_features:\n",
        "            return x\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fo1QJ8aIod4"
      },
      "source": [
        "# Section 3: Load, augment, preprocess, and split the dataset\n",
        "- Augment data with techniques such as random rotations, translations, and flips.\n",
        "- Preprocess and split the dataset into labeled, unlabeled, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "i4sPDD9fJF6f"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(32), \n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "crop_augmentation = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomResizedCrop(24),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "gaussian_blur_augmentation = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "gaussian_noise_augmentation = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x : x + math.sqrt(0.1)*torch.randn_like(x))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xLgWn-A4JFlY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:04<00:00, 2440061.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 103760.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 2566858.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "mnist_train = datasets.MNIST(root='./data', train=True, download=True,\n",
        "                                transform=transform)\n",
        "mnist_train_crop = datasets.MNIST(root='./data', train=True, download=False, \n",
        "                                transform=gaussian_blur_augmentation)\n",
        "mnist_train_blur = datasets.MNIST(root='./data', train=True, download=False, \n",
        "                                transform=gaussian_noise_augmentation)\n",
        "mnist_train_noise = datasets.MNIST(root='./data', train=True, download=False, \n",
        "                                transform=transform)\n",
        "\n",
        "augmented_trainset = torch.utils.data.ConcatDataset([\n",
        "    mnist_train, \n",
        "    mnist_train_crop, \n",
        "    mnist_train_blur, \n",
        "    mnist_train_noise])\n",
        "\n",
        "mnist_test = datasets.MNIST(root='./data', train=False, download=True,\n",
        "                               transform=transform)\n",
        "mnist_test_crop = datasets.MNIST(root='./data', train=False, download=False, \n",
        "                                transform=gaussian_blur_augmentation)\n",
        "mnist_test_blur = datasets.MNIST(root='./data', train=False, download=False, \n",
        "                                transform=gaussian_noise_augmentation)\n",
        "mnist_test_noise = datasets.MNIST(root='./data', train=False, download=False, \n",
        "                                transform=transform)\n",
        "\n",
        "augmented_testset = torch.utils.data.ConcatDataset([\n",
        "    mnist_test, \n",
        "    mnist_test_crop, \n",
        "    mnist_test_blur, \n",
        "    mnist_test_noise])\n",
        "\n",
        "labeled_size = int(LABELED_RATIO * len(augmented_trainset))\n",
        "unlabeled_size = len(augmented_trainset) - labeled_size\n",
        "labeled_augmented_trainset, unlabeled_augmented_trainset = torch.utils.data.random_split(augmented_trainset, [labeled_size, unlabeled_size])\n",
        "\n",
        "fully_labeled_loader = torch.utils.data.DataLoader(augmented_trainset, BATCH, shuffle=True)\n",
        "\n",
        "labeled_loader = torch.utils.data.DataLoader(labeled_augmented_trainset, BATCH, shuffle=True)\n",
        "unlabeled_loader = torch.utils.data.DataLoader(unlabeled_augmented_trainset, BATCH, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(augmented_testset, 1, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrRAwSJPIoIt"
      },
      "source": [
        "# Section 6: Define the training and evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "KrFk-2OPJGhC"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, opt, loss_fn):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "\n",
        "    for batch, (train, label) in enumerate(dataloader):\n",
        "        train, label = train.to(DEVICE), label.to(DEVICE)\n",
        "        opt.zero_grad()\n",
        "        output = model(train)\n",
        "        loss = loss_fn(output, label)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        _, predicted = torch.max(output, dim=1)\n",
        "        correct += (predicted == label).float().sum().item()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        train_loss.append(loss)\n",
        "        train_acc.append(100 * correct / ((batch + 1) * BATCH))\n",
        "\n",
        "    accuracy = 100 * correct / len(dataloader.dataset)\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "\n",
        "    return average_loss, accuracy, train_loss, train_acc\n",
        "\n",
        "def evaluate(model, dataloader, loss_fn):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    test_loss = []\n",
        "    test_acc = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (test, label) in enumerate(dataloader):\n",
        "            test, label = test.to(DEVICE), label.to(DEVICE)\n",
        "            output = model(test)\n",
        "            loss = loss_fn(output, label)\n",
        "\n",
        "            _, predicted = torch.max(output, dim=1)\n",
        "            correct += (predicted == label).float().sum().item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            test_loss.append(loss)\n",
        "            test_acc.append(100 * correct / (i + 1))\n",
        "\n",
        "    accuracy = 100 * correct / len(dataloader.dataset)\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "\n",
        "    return average_loss, accuracy, test_loss, test_acc\n",
        "\n",
        "# Extract deep features from the given model\n",
        "def extract_deep_features(model, dataloader):\n",
        "    model.eval()\n",
        "    features_list = []\n",
        "    label_list = []\n",
        "    with torch.no_grad():\n",
        "        for train, label in dataloader:\n",
        "            train = train.to(DEVICE)\n",
        "            features = model.forward(train, extract_features=True)\n",
        "            features_list.append(features.cpu())\n",
        "            label_list.append(label)\n",
        "\n",
        "    features_tensor = torch.cat(features_list)\n",
        "    labels_tensor = torch.cat(label_list)\n",
        "    return features_tensor, labels_tensor\n",
        "\n",
        "# Perform K-means clustering on the given data\n",
        "def k_means_clustering(data, n_clusters):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
        "    kmeans.fit(data)\n",
        "    return kmeans\n",
        "\n",
        "# Assign pseudo-labels to the unlabeled data using KNN\n",
        "def knn_only_labeling(labeled_features, labels, unlabeled_features, k):\n",
        "    labeled_broadcast = LazyTensor(labeled_features.unsqueeze(0))\n",
        "    neighbors = LazyTensor(unlabeled_features.unsqueeze(1))\n",
        "\n",
        "    # [unlabeled_features x labeled_features] squared L2 distances\n",
        "    L2_dist = ((labeled_broadcast - neighbors) ** 2).sum(-1)\n",
        "    knn_indices = L2_dist.argKmin(k, dim=1)\n",
        "    knn_labels = labels[knn_indices]\n",
        "\n",
        "    pseudo_labels, _ = knn_labels.mode(dim=1)\n",
        "    return pseudo_labels\n",
        "\n",
        "def knn_only_batch_labeling(labeled_features, labels, unlabeled_features, k):\n",
        "\n",
        "    KNN_batch_loader = torch.utils.data.DataLoader(unlabeled_features, BATCH, shuffle=False)\n",
        "    pseudo_labels = []\n",
        "    labeled_broadcast = labeled_features.unsqueeze(0)\n",
        "\n",
        "    for batch, unlabeled in enumerate(KNN_batch_loader):\n",
        "        neighbors = unlabeled.unsqueeze(1)\n",
        "\n",
        "        # [unlabeled_features x labeled_features] squared L2 distances\n",
        "        L2_dist = torch.sum((labeled_broadcast - neighbors) ** 2, dim=-1)\n",
        "        _, knn_indices = torch.topk(L2_dist, k, dim=1, largest=False)\n",
        "        knn_labels = labels[knn_indices]\n",
        "\n",
        "        batch_labels, _ = knn_labels.mode(dim=1)\n",
        "        pseudo_labels.append(batch_labels)\n",
        "    \n",
        "    torch.cat(pseudo_labels)\n",
        "    return pseudo_labels\n",
        "\n",
        "def knn_clustering_labeling(labeled_features, labels, unlabeled_features, k=5, n_clusters=10):\n",
        "    kmeans_model = k_means_clustering(unlabeled_features, n_clusters)\n",
        "    pseudo_labels = torch.zeros(len(unlabeled_augmented_trainset))\n",
        "\n",
        "    for cluster_index in range(n_clusters):\n",
        "        cluster_indices = torch.nonzero(\n",
        "            torch.from_numpy(kmeans_model.labels_ == cluster_index)).squeeze()\n",
        "        \n",
        "        if (USE_SYMBOLIC):\n",
        "            # using entire cluster in KNN\n",
        "            cluster_features = unlabeled_features[cluster_indices]\n",
        "            knn_labels = knn_only_labeling(labeled_features, labels, cluster_features, k=k)\n",
        "        else:\n",
        "            # use a subset of the KNN cluster for efficiency on CPU\n",
        "            cluster_indices = cluster_indices[:int(CLUSTER_RATIO * len(cluster_indices))]\n",
        "            cluster_features = unlabeled_features[cluster_indices]\n",
        "            knn_labels = knn_only_batch_labeling(labeled_features, labels, cluster_features, k=k)\n",
        "\n",
        "        # Assign the majority label to the entire cluster\n",
        "        cluster_label = mode(knn_labels)\n",
        "        pseudo_labels[cluster_indices] = cluster_label\n",
        "\n",
        "    return pseudo_labels\n",
        "\n",
        "# Merge labeled and pseudo-labeled data\n",
        "def merge_labeled_and_pseudo_labeled_data(labeled_loader, unlabeled_loader, pseudo_labels):\n",
        "    # Extract the labeled dataset and replace the targets with pseudo-labels\n",
        "    labeled_dataset = labeled_loader.dataset\n",
        "    unlabeled_dataset = unlabeled_loader.dataset\n",
        "    unlabeled_dataset.targets = pseudo_labels.tolist()\n",
        "\n",
        "    # Combine the labeled and unlabeled datasets and create a new dataloader\n",
        "    combined_dataset = torch.utils.data.ConcatDataset([labeled_dataset, unlabeled_dataset])\n",
        "    combined_loader = torch.utils.data.DataLoader(combined_dataset, batch_size=BATCH, shuffle=True)\n",
        "\n",
        "    return combined_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny0z4f0-InxR"
      },
      "source": [
        "# Section 7: Train, evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "X56NIJhUut-Y"
      },
      "outputs": [],
      "source": [
        "def train_and_test(train_loader, test_loader, model_name):\n",
        "    set_device()\n",
        "    net = Net()\n",
        "    net.to(DEVICE)\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "    for epoch in range(EPOCH):\n",
        "        avg_train_loss, avg_train_acc, train_loss, train_acc = train(net, train_loader, optimizer, loss)\n",
        "        avg_test_loss, avg_test_acc, test_loss, test_acc = evaluate(net, test_loader, loss)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{EPOCH}\")\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_acc:.2f}%\")\n",
        "        print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {avg_test_acc:.2f}%\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    torch.save(net.state_dict(), model_name)\n",
        "    return net, train_loss, train_acc, test_loss, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djZKZ_soJGwr",
        "outputId": "8351b424-601a-4fbc-e581-99e32193607d"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m net, _, _, _, _ \u001b[39m=\u001b[39m train_and_test(labeled_loader, test_loader, \u001b[39m'\u001b[39;49m\u001b[39mencoder.pth\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m \u001b[39m# Extract deep features from the supervised model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m deep_features_labeled, labels \u001b[39m=\u001b[39m extract_deep_features(net, labeled_loader)\n",
            "Cell \u001b[1;32mIn[27], line 8\u001b[0m, in \u001b[0;36mtrain_and_test\u001b[1;34m(train_loader, test_loader, model_name)\u001b[0m\n\u001b[0;32m      6\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(net\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCH):\n\u001b[1;32m----> 8\u001b[0m     avg_train_loss, avg_train_acc, train_loss, train_acc \u001b[39m=\u001b[39m train(net, train_loader, optimizer, loss)\n\u001b[0;32m      9\u001b[0m     avg_test_loss, avg_test_acc, test_loss, test_acc \u001b[39m=\u001b[39m evaluate(net, test_loader, loss)\n\u001b[0;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mEPOCH\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[26], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, opt, loss_fn)\u001b[0m\n\u001b[0;32m      5\u001b[0m train_loss \u001b[39m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m train_acc \u001b[39m=\u001b[39m []\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfor\u001b[39;00m batch, (train, label) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m      9\u001b[0m     train, label \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39mto(DEVICE), label\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m     10\u001b[0m     opt\u001b[39m.\u001b[39mzero_grad()\n",
            "File \u001b[1;32mc:\\Users\\bowen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\bowen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\bowen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
            "File \u001b[1;32mc:\\Users\\bowen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:264\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    204\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
            "File \u001b[1;32mc:\\Users\\bowen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\bowen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\bowen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
            "File \u001b[1;32mc:\\Users\\bowen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# net, _, _, _, _ = train_and_test(labeled_loader, test_loader, 'encoder.pth')\n",
        "\n",
        "# Extract deep features from the supervised model\n",
        "deep_features_labeled, labels = extract_deep_features(net, labeled_loader)\n",
        "deep_features_unlabeled, _ = extract_deep_features(net, unlabeled_loader)\n",
        "\n",
        "# Assign pseudo-labels to the clustered data using KNN \n",
        "pseudo_labels = knn_clustering_labeling(deep_features_labeled, labels, deep_features_unlabeled, k=5, n_clusters=10)\n",
        "\n",
        "# Merge labeled and pseudo-labeled data\n",
        "combined_loader = merge_labeled_and_pseudo_labeled_data(labeled_loader, unlabeled_loader, pseudo_labels)\n",
        "\n",
        "print(f\"Fully-supervised MNIST training\")\n",
        "_, fs_train_loss, fs_train_acc, fs_test_loss, fs_test_acc = train_and_test(fully_labeled_loader, test_loader, 'fully-supervised.pth')\n",
        "print(f\"Semi-supervised MNIST training\")\n",
        "_, ss_train_loss, ss_train_acc, ss_test_loss, ss_test_acc = train_and_test(combined_loader, test_loader, 'semi-supervised.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXrVoFJQI_h2"
      },
      "source": [
        "# Section 8: Comparison of the methods\n",
        "\n",
        "- Self training\n",
        "- PCA + KNN\n",
        "- ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMfe2SRhI_WT"
      },
      "source": [
        "# Section 9: Visualize the results and save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "J604YzreJHWR"
      },
      "outputs": [],
      "source": [
        "plt.figure(1)\n",
        "x_axis = np.arange(len(fs_train_loss))\n",
        "y_axis = fs_train_loss\n",
        "plt.xlabel('Batch number')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(x_axis, y_axis)\n",
        "\n",
        "plt.figure(2)\n",
        "x_axis = np.arange(len(fs_train_acc))\n",
        "y_axis = fs_train_acc\n",
        "plt.xlabel('Batch number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(x_axis, y_axis)\n",
        "\n",
        "plt.figure(3)\n",
        "x_axis = np.arange(len(fs_test_loss))\n",
        "y_axis = fs_test_loss\n",
        "plt.xlabel('Test point')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(x_axis, y_axis)\n",
        "\n",
        "plt.figure(4)\n",
        "x_axis = np.arange(len(fs_test_acc))\n",
        "y_axis = fs_test_acc\n",
        "plt.xlabel('Test point')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(x_axis, y_axis)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kypZsjJKgtWW"
      },
      "source": [
        "# Section 10: Conclusion and final thoughts"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
