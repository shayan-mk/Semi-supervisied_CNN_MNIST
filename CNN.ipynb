{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shayan-mk/Semi-supervisied_CNN_MNIST/blob/master/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "t2TQIRNrOVdO"
      },
      "source": [
        "# Section 0: Introduction"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Abstract:\n",
        "\n",
        "The objective of this project is to demonstrate the effect of semi-supervised image classification over its fully supervised counterpart. Using the MNIST dataset of handwritten digits, the motivation being to show the benefits of having unlabeled data points in the training dataset. Using a convolutional neural network trained over a subset of the labeled MNIST dataset, we leverage its learned associations as an encoder to extract deep features from the remaining subset of the labeled dataset. In the CNN, we use a series of convolutional layers to expose low level features before flattening them for the linear classifier. Applying unsupervised K-means clustering over the unlabeled subset of training data, we introduce a clustering loss factor to these data points in relation to the ground truth label. Using the K-nearest neighbors algorithms, we propagate the set of assignable labels (0-9) over the 10 clusters. The CNN is then trained over the full set of ground-truth and generated-label datapoints. The goal then is to show the varying degrees of effectiveness between fully labeled and different ratios of unlabeled-to-labeled datasets. Illustrated and graphed by the change in loss and accuracy at each batch across all epochs for both the training and test MNIST datasets."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Team members and contributions:\n",
        "\n",
        "- Bowen Luo (b23luo@uwaterloo.ca)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "M4iBebg4ut-R"
      },
      "source": [
        "# Code outline:\n",
        "\n",
        "- Apply data augmentation techniques to expand the training dataset with rotation, cropping, blurring, and noise transformations to reduce overfitting.\n",
        "- Train the initial fully supervised convolutional neural network on the labeled subset of MNIST data.\n",
        "- Extract deep features from the last layer before the linear classifier for the unlabeled data points.\n",
        "- Perform K-means clustering on the deep features to assign cluster labels to the unlabeled data points.\n",
        "- Use K-Nearest Neighbors (KNN) on the deep features to further refine the labels assigned by K-means clustering based on the labels of their nearest neighbors in the feature space.\n",
        "- Combine the labeled, K-means labeled, and KNN-refined labeled data points into a single dataset.\n",
        "- Fine-tune the neural network using this dataset with the combined loss function.\n",
        "\n",
        "Finally, Evaluate the model performance with varying a variying ratio of labeled data to demonstrate how the performance changes as the ratio gets progressively smaller. We will do this by repeating the above steps for different proportions of the training MNIST dataset."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FaCfyyhLIFYY"
      },
      "source": [
        "# Section 1: Code libraries\n",
        "\n",
        "Aside from essential libraries such as math and numpy, pytorch is used for its tensors, transforms, dataset objects, as well as general CNN related functions including loss functions, convolutional layers, and forward/backward passes over the CNN. Matplotlib is used to graph the loss and accuracies over the batches, sklearn is used for K-means clustering, and we use KeOps for its conversion of tensors into symbolic variables for low overhead matrix operations for K-nearest-neighbors.\n",
        "\n",
        "Note: running on colab <strong>using GPU</strong>, the following code cell is included for installation of KeOps (pykeops). Locally, it requires the CUDA toolkit and compatible g++ compiler. Further installation requirements can be found here: https://www.kernel-operations.io/keops/python/installation.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MGJh5-zaTa3",
        "outputId": "43fa0e74-7c02-4cef-d91a-4f2150d51494"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pykeops\n",
            "  Downloading pykeops-2.1.2.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.9/88.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pykeops) (1.22.4)\n",
            "Collecting pybind11\n",
            "  Downloading pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.3/222.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keopscore==2.1.2\n",
            "  Downloading keopscore-2.1.2.tar.gz (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pykeops, keopscore\n",
            "  Building wheel for pykeops (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pykeops: filename=pykeops-2.1.2-py3-none-any.whl size=114095 sha256=0b4de0d03785904f81ffb5c7a90aa020a10d7e5444c11d10112e8d92ef685daf\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/a1/b9/156d48595487f5df04d1c5eee5ec01f3330ae2debece7ca6fe\n",
            "  Building wheel for keopscore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keopscore: filename=keopscore-2.1.2-py3-none-any.whl size=146469 sha256=175c2f4ac9bb8997a60f87b47c2f204f51bff3652d3b8158e4557d8cac9f7d4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/53/76/4a583149d55824e0d0a30a303cc5cd6ae1d4545d3f61f346c9\n",
            "Successfully built pykeops keopscore\n",
            "Installing collected packages: pybind11, keopscore, pykeops\n",
            "Successfully installed keopscore-2.1.2 pybind11-2.10.4 pykeops-2.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pykeops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcqxDZyYIatw",
        "outputId": "c45aeba2-07fd-4bf2-8d07-5bec3fef37b9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from pykeops.torch import LazyTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8ZdSNfgSLaVv"
      },
      "outputs": [],
      "source": [
        "USE_GPU = True\n",
        "EPOCH = 2\n",
        "BATCH = 60\n",
        "LABELED_RATIO = 0.2\n",
        "DEVICE = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VZftz1ACut-U"
      },
      "outputs": [],
      "source": [
        "def set_device():\n",
        "    global DEVICE\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-P_1ocVIonk"
      },
      "source": [
        "# Section 2: Define the CNN architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GC8Bd7ouJFUe"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    # create convolutional and batch layers 1 through 8 in constructor\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, 3, padding=1)\n",
        "        self.conv1_bn = nn.BatchNorm2d(64)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv2_bn = nn.BatchNorm2d(128)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.conv3_bn = nn.BatchNorm2d(256)\n",
        "        \n",
        "        self.conv4 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "        self.conv4_bn = nn.BatchNorm2d(256)\n",
        "        \n",
        "        self.conv5 = nn.Conv2d(256, 512, 3, padding=1)\n",
        "        self.conv5_bn = nn.BatchNorm2d(512)\n",
        "        \n",
        "        self.conv6 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "        self.conv6_bn = nn.BatchNorm2d(512)\n",
        "        \n",
        "        self.conv7 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "        self.conv7_bn = nn.BatchNorm2d(512)\n",
        "        \n",
        "        self.conv8 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "        self.conv8_bn = nn.BatchNorm2d(512)\n",
        "        # create fully connected layers and dropout\n",
        "        self.fc1 = nn.Linear(512, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, 10)\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    # define forward pass through network, conversely, could use nn.sequential\n",
        "    def forward(self, x, extract_features=False):\n",
        "        x = F.max_pool2d(F.relu(self.conv1_bn(self.conv1(x))), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2_bn(self.conv2(x))), (2, 2))\n",
        "        x = F.relu(self.conv3_bn(self.conv3(x)))\n",
        "        x = F.max_pool2d(F.relu(self.conv4_bn(self.conv4(x))), (2, 2))\n",
        "        x = F.relu(self.conv5_bn(self.conv5(x)))\n",
        "        x = F.max_pool2d(F.relu(self.conv6_bn(self.conv6(x))), (2, 2))\n",
        "        x = F.relu(self.conv7_bn(self.conv7(x)))\n",
        "        x = F.max_pool2d(F.relu(self.conv8_bn(self.conv8(x))), (2, 2))\n",
        "        \n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        if extract_features:\n",
        "            return x\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fo1QJ8aIod4"
      },
      "source": [
        "# Section 3: Load, augment, preprocess, and split the dataset\n",
        "- Augment data with techniques such as random rotations, translations, and flips.\n",
        "- Preprocess and split the dataset into labeled, unlabeled, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "i4sPDD9fJF6f"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(32), \n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "crop_augmentation = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomResizedCrop(24),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "gaussian_blur_augmentation = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "gaussian_noise_augmentation = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x : x + math.sqrt(0.1)*torch.randn_like(x))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xLgWn-A4JFlY"
      },
      "outputs": [],
      "source": [
        "mnist_train = datasets.MNIST(root='./data', train=True, download=True,\n",
        "                                transform=transform)\n",
        "mnist_train_crop = datasets.MNIST(root='./data', train=True, download=False, \n",
        "                                transform=gaussian_blur_augmentation)\n",
        "mnist_train_blur = datasets.MNIST(root='./data', train=True, download=False, \n",
        "                                transform=gaussian_noise_augmentation)\n",
        "mnist_train_noise = datasets.MNIST(root='./data', train=True, download=False, \n",
        "                                transform=transform)\n",
        "\n",
        "augmented_trainset = torch.utils.data.ConcatDataset([\n",
        "    mnist_train, \n",
        "    mnist_train_crop, \n",
        "    mnist_train_blur, \n",
        "    mnist_train_noise])\n",
        "\n",
        "mnist_test = datasets.MNIST(root='./data', train=False, download=True,\n",
        "                               transform=transform)\n",
        "mnist_test_crop = datasets.MNIST(root='./data', train=False, download=False, \n",
        "                                transform=gaussian_blur_augmentation)\n",
        "mnist_test_blur = datasets.MNIST(root='./data', train=False, download=False, \n",
        "                                transform=gaussian_noise_augmentation)\n",
        "mnist_test_noise = datasets.MNIST(root='./data', train=False, download=False, \n",
        "                                transform=transform)\n",
        "\n",
        "augmented_testset = torch.utils.data.ConcatDataset([\n",
        "    mnist_test, \n",
        "    mnist_test_crop, \n",
        "    mnist_test_blur, \n",
        "    mnist_test_noise])\n",
        "\n",
        "labeled_size = int(LABELED_RATIO * len(augmented_trainset))\n",
        "unlabeled_size = len(augmented_trainset) - labeled_size\n",
        "labeled_augmented_trainset, unlabeled_augmented_trainset = torch.utils.data.random_split(augmented_trainset, [labeled_size, unlabeled_size])\n",
        "\n",
        "fully_labeled_loader = torch.utils.data.DataLoader(augmented_trainset, BATCH, shuffle=True)\n",
        "\n",
        "labeled_loader = torch.utils.data.DataLoader(labeled_augmented_trainset, BATCH, shuffle=True)\n",
        "unlabeled_loader = torch.utils.data.DataLoader(unlabeled_augmented_trainset, BATCH, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(augmented_testset, 1, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrRAwSJPIoIt"
      },
      "source": [
        "# Section 6: Define the training and evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KrFk-2OPJGhC"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, opt, loss_fn):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "\n",
        "    for batch, (train, label) in enumerate(dataloader):\n",
        "        train, label = train.to(DEVICE), label.to(DEVICE)\n",
        "        opt.zero_grad()\n",
        "        output = model(train)\n",
        "        loss = loss_fn(output, label)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        _, predicted = torch.max(output, dim=1)\n",
        "        correct += (predicted == label).float().sum().item()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        train_loss.append(loss)\n",
        "        train_acc.append(100 * correct / ((batch + 1) * BATCH))\n",
        "\n",
        "    accuracy = 100 * correct / len(dataloader.dataset)\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "\n",
        "    return average_loss, accuracy, train_loss, train_acc\n",
        "\n",
        "def evaluate(model, dataloader, loss_fn):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    test_loss = []\n",
        "    test_acc = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (test, label) in enumerate(dataloader):\n",
        "            test, label = test.to(DEVICE), label.to(DEVICE)\n",
        "            output = model(test)\n",
        "            loss = loss_fn(output, label)\n",
        "\n",
        "            _, predicted = torch.max(output, dim=1)\n",
        "            correct += (predicted == label).float().sum().item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            test_loss.append(loss)\n",
        "            test_acc.append(100 * correct / (i + 1))\n",
        "\n",
        "    accuracy = 100 * correct / len(dataloader.dataset)\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "\n",
        "    return average_loss, accuracy, test_loss, test_acc\n",
        "\n",
        "# Extract deep features from the given model\n",
        "def extract_deep_features(model, dataloader):\n",
        "    model.eval()\n",
        "    features_list = []\n",
        "    label_list = []\n",
        "    with torch.no_grad():\n",
        "        for train, label in dataloader:\n",
        "            train = train.to(DEVICE)\n",
        "            features = model.forward(train, extract_features=True)\n",
        "            features_list.append(features.cpu())\n",
        "            label_list.append(label)\n",
        "\n",
        "    features_tensor = torch.cat(features_list)\n",
        "    labels_tensor = torch.cat(label_list)\n",
        "    return features_tensor, labels_tensor\n",
        "\n",
        "# Perform K-means clustering on the given data\n",
        "def k_means_clustering(data, n_clusters):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
        "    kmeans.fit(data)\n",
        "    return kmeans\n",
        "\n",
        "# Assign pseudo-labels to the unlabeled data using KNN\n",
        "def knn_only_labeling(labeled_features, labels, unlabeled_features, k):\n",
        "    labeled_broadcast = LazyTensor(labeled_features.unsqueeze(0))\n",
        "    neighbors = LazyTensor(unlabeled_features.unsqueeze(1))\n",
        "\n",
        "    # [unlabeled_features x labeled_features] squared L2 distances\n",
        "    L2_dist = ((labeled_broadcast - neighbors) ** 2).sum(-1)\n",
        "    knn_indices = L2_dist.argKmin(k, dim=1)\n",
        "    knn_labels = labels[knn_indices]\n",
        "\n",
        "    pseudo_labels, _ = knn_labels.mode(dim=1)\n",
        "    return pseudo_labels\n",
        "\n",
        "def knn_clustering_labeling(labeled_features, labels, unlabeled_features, k=5, n_clusters=10):\n",
        "    kmeans_model = k_means_clustering(unlabeled_features, n_clusters)\n",
        "    pseudo_labels = torch.zeros(len(unlabeled_augmented_trainset))\n",
        "\n",
        "    for cluster_index in range(n_clusters):\n",
        "        cluster_indices = torch.nonzero(\n",
        "            torch.from_numpy(kmeans_model.labels_ == cluster_index)).squeeze()\n",
        "        cluster_features = unlabeled_features[cluster_indices]\n",
        "        knn_labels = knn_only_labeling(labeled_features, labels, cluster_features, k=k)\n",
        "\n",
        "        # Assign the majority label to the entire cluster\n",
        "        cluster_label, _ = knn_labels.mode()\n",
        "        pseudo_labels[cluster_indices] = cluster_label.item()\n",
        "\n",
        "    return pseudo_labels\n",
        "\n",
        "# Merge labeled and pseudo-labeled data\n",
        "def merge_labeled_and_pseudo_labeled_data(labeled_loader, unlabeled_loader, pseudo_labels):\n",
        "    # Extract the labeled dataset and replace the targets with pseudo-labels\n",
        "    labeled_dataset = labeled_loader.dataset\n",
        "    unlabeled_dataset = unlabeled_loader.dataset\n",
        "    unlabeled_dataset.targets = pseudo_labels.tolist()\n",
        "\n",
        "    # Combine the labeled and unlabeled datasets and create a new dataloader\n",
        "    combined_dataset = torch.utils.data.ConcatDataset([labeled_dataset, unlabeled_dataset])\n",
        "    combined_loader = torch.utils.data.DataLoader(combined_dataset, batch_size=BATCH, shuffle=True)\n",
        "\n",
        "    return combined_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny0z4f0-InxR"
      },
      "source": [
        "# Section 7: Train, evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "X56NIJhUut-Y"
      },
      "outputs": [],
      "source": [
        "def train_and_test(train_loader, test_loader, model_name):\n",
        "    set_device()\n",
        "    net = Net()\n",
        "    net.to(DEVICE)\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "    for epoch in range(EPOCH):\n",
        "        avg_train_loss, avg_train_acc, train_loss, train_acc = train(net, train_loader, optimizer, loss)\n",
        "        avg_test_loss, avg_test_acc, test_loss, test_acc = evaluate(net, test_loader, loss)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{EPOCH}\")\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_acc:.2f}%\")\n",
        "        print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {avg_test_acc:.2f}%\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    torch.save(net.state_dict(), model_name)\n",
        "    return net, train_loss, train_acc, test_loss, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djZKZ_soJGwr",
        "outputId": "8351b424-601a-4fbc-e581-99e32193607d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "Train Loss: 0.5117, Train Accuracy: 83.38%\n",
            "Test Loss: 0.1532, Test Accuracy: 95.04%\n",
            "------------------------------\n",
            "Epoch 2/2\n",
            "Train Loss: 0.1476, Train Accuracy: 95.42%\n",
            "Test Loss: 0.1392, Test Accuracy: 95.42%\n",
            "------------------------------\n",
            "[KeOps] Generating code for formula ArgKMin_Reduction(Sum((Var(0,512,1)-Var(1,512,0))**2),0) ... OK\n",
            "Epoch 1/2\n",
            "Train Loss: 0.1936, Train Accuracy: 93.81%\n",
            "Test Loss: 0.0920, Test Accuracy: 97.10%\n",
            "------------------------------\n",
            "Epoch 2/2\n",
            "Train Loss: 0.0651, Train Accuracy: 97.96%\n",
            "Test Loss: 0.0631, Test Accuracy: 97.89%\n",
            "------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv1_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv4_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv5_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv6_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv7_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv8_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc1): Linear(in_features=512, out_features=4096, bias=True)\n",
              "  (fc2): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "  (fc3): Linear(in_features=4096, out_features=10, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net, _, _, _, _ = train_and_test(labeled_loader, test_loader, 'encoder.pth')\n",
        "\n",
        "# Extract deep features from the supervised model\n",
        "deep_features_labeled, labels = extract_deep_features(net, labeled_loader)\n",
        "deep_features_unlabeled, _ = extract_deep_features(net, unlabeled_loader)\n",
        "\n",
        "# Assign pseudo-labels to the clustered data using KNN \n",
        "pseudo_labels = knn_clustering_labeling(deep_features_labeled, labels, deep_features_unlabeled, k=5, n_clusters=10)\n",
        "\n",
        "# Merge labeled and pseudo-labeled data\n",
        "combined_loader = merge_labeled_and_pseudo_labeled_data(labeled_loader, unlabeled_loader, pseudo_labels)\n",
        "\n",
        "print(f\"Fully-supervised MNIST training\")\n",
        "_, fs_train_loss, fs_train_acc, fs_test_loss, fs_test_acc = train_and_test(fully_labeled_loader, test_loader, 'fully-supervised.pth')\n",
        "print(f\"Semi-supervised MNIST training\")\n",
        "_, ss_train_loss, ss_train_acc, ss_test_loss, ss_test_acc = train_and_test(combined_loader, test_loader, 'semi-supervised.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXrVoFJQI_h2"
      },
      "source": [
        "# Section 8: Comparison of the methods\n",
        "\n",
        "- Self training\n",
        "- PCA + KNN\n",
        "- ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMfe2SRhI_WT"
      },
      "source": [
        "# Section 9: Visualize the results and save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "J604YzreJHWR"
      },
      "outputs": [],
      "source": [
        "plt.figure(1)\n",
        "x_axis = np.arange(len(fs_train_loss))\n",
        "y_axis = fs_train_loss\n",
        "plt.xlabel('Batch number')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(x_axis, y_axis)\n",
        "\n",
        "plt.figure(2)\n",
        "x_axis = np.arange(len(fs_train_acc))\n",
        "y_axis = fs_train_acc\n",
        "plt.xlabel('Batch number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(x_axis, y_axis)\n",
        "\n",
        "plt.figure(3)\n",
        "x_axis = np.arange(len(fs_test_loss))\n",
        "y_axis = fs_test_loss\n",
        "plt.xlabel('Test point')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(x_axis, y_axis)\n",
        "\n",
        "plt.figure(4)\n",
        "x_axis = np.arange(len(fs_test_acc))\n",
        "y_axis = fs_test_acc\n",
        "plt.xlabel('Test point')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(x_axis, y_axis)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kypZsjJKgtWW"
      },
      "source": [
        "# Section 10: Conclusion and final thoughts"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
